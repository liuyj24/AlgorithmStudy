### 布隆过滤器
##### 先撇开结构, 了解作用及优劣
- 应用场景 : 浏览器要做一个儿童黑名单系统, 把儿童不适应看的网站的url收集起来, 当用户访问的时候发现是黑名单中的url, 则不予以通过. 假设黑名单系统中的url(大小为64字节)有一百多亿, 直接用哈希表则需要640g的内存, 我们需要用一种别的方法实现这样的功能 :   
a) 往黑名单中添加url  
b) 判断url是不是属于黑名单中
- 可以使用布隆过滤器实现, 布隆过滤器能大大减小内存空间的占用. 但是布隆过滤器一定会存在失误率, 怎么个失误法?比如说   
A url存在于黑名单中, 布隆过滤器一定不会失误, 肯定能拦截掉  
B url不存在于黑名单中, 布隆过滤器可能会失误地把它拦截掉  
但失误率很低, 我们可以通过设计和增加内存空间减少失误率. 
- 前置知识 : 位图   
就是以比特为单位的数组, 我们用过int, String类型的数组, 现在我们要用以每个bit为元素的数组. 如何得到? 用int类型数组拼出来, 一个int类型为32bit, 创建一个长度为1的int类型数组, 也就等于创建了一个长度为32的bit类型数组. 
---
##### 实现思路
- 假设开的位图的大小为m
- 现在有3的url被列为黑名单, 准备了k个独立的哈希函数去计算这些url. 
- url1 经过哈希函数h1计算后得到哈希值c1, `z1 % m` 后得到的一个 0~m 上的位置, (位图上每个元素的值不是0就是1), 如果该位置上的值为0 就置为1, 如果是1那么仍保持1. 
- url1 继续通过下一个哈希函数h2计算后往位图上描点. 
- url1 计算完, 位图上应该有k个点了, 然后url2, url3 也是这么跑一遍. 黑名单系统就构成了. 
- 那么如何去过滤用户访问的url是否属于黑名单系统, 也就是去查询某个url是否属于黑名单呢?  
- 我们把用户的url按照k个独立的哈希函数也算一遍, 就能得到k个位置, 然后拿着k个位置逐个到位图上比对, 如果k个位置对应的全是黑点(置1), 证明是黑名单, 只要有一个位置没有置1证明不是黑名单. 
---
##### 深入
- 单样本的大小和布隆过滤器的设计有没有关系, 比如说有些url的长度不是64字节, 是128字节.  
没有关系. 只要url能跑那k个哈希函数就可以了. 保证哈希函数能接收单样本即可, 要的只是哈希函数计算得出的hashCode
- 所以在面对具体问题的时候我们只需要知道两个参数, 一个样本量n(而不是单样本大小), 另一个是布隆过滤器能容忍的最大失误率是多少? 

> 下面我们在一个具体问题中应用一次布隆过滤器.  
有一个黑名单系统, 里面存储了100亿条url, 每条url大小为64字节, 问这个黑名单要怎么存? 如果此时随便输入一个url, 如何判断这个url是否在这个黑名单中?

- 使用哈希表是不可能的了, 10亿条64字节的url要用`64 * 2^30`也就是64g的内存空间. 我们使用布隆过滤器. 
- 现在我们有了样本量n, 还需要容错率, 我们假定失误率是万分之一. 现在我们有了n `(6.4 * 2^30)Byte为单位` 和 p `个数为单位`, 可以通过公式计算位图的大小m了.
- 位图开多大 : `m = - (n * lnp) / (ln2)^2 = 120 * 2^30 (bit)`, 计算得出位图需要开`(120 * 2^30) / 8 (Byte) = 15 * 2^30(GB)` 也就是15GB的大小. 
- 下一步我们将使用位图的大小和样本量计算需要多少个哈希函数. 
- 哈希函数的数量 : `k = ln2 * (m / n) = 19.2`个.
- 但是如果题目允许我们使用的内存空间不只15GB, 那我们大可不必局限于15GB, 给位图开更大的空间能降低容错率嘛. 假如给了我们32GB, 那我们用24GB, 哈希函数我们也不那么抠门, 我们直接使用20个. 
- 接下来我们可以重新根据样本量n和哈希函数数量k计算失误率p. 这时候m的值为`24 * 2^30 * 8`
- 失误率`p = (1 - e^(- nk/m)) ^ k = 6.1e-7` 
- 最终会计算出一个新的失误率6.1乘10的-7次方, 远比原来的失误率小. 这就是结果. 

> 最后我们总结一下布隆过滤器, 引用维基百科中的解释 : 从本质上说布隆过滤器就是一个很大的二进制向量和一堆随机映射函数. 它的优点是空间效率和查询时间都远远超出一般的算法. 它的缺点是必须容忍一定的失误率, 并且删除困难.   
像类似上面的黑名单题目, 用布隆过滤器解比用哈希表解决能节省1/4~1/8的空间.

- 补充一点, 上面那个题目样本量本来是100亿个url的, 好像说算出来是26GB左右, 也就是640GB降到26GB, 不知道是不是我算错了. 但是对于工程级别的服务器来说26GB根本不是事情. 失误率是十万分之六.
- 所以像网页爬虫, 黑名单这样的问题, 用hash表太浪费空间了, 如果得知可以容忍失误率, 那就上布隆过滤器.





